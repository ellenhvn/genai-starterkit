{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pypdf\n",
    "# !pip install InstructorEmbedding\n",
    "# !pip install faiss-cpu\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import RetrievalQA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PDF Using PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we use PyPDFLoader but LangChain offers other loaders too (for PDF or other unstructured data)\n",
    "loader = PyPDFLoader(\"./data/9001408066_B.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 document(s) in data\n"
     ]
    }
   ],
   "source": [
    "#every page in pdf is counted as unique document\n",
    "print (f'{len(data)} document(s) in data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Laundry    en\\n11ZLaundry\\nLaundryPreparing the laundry\\nCaution!\\nDamage to the appliance/fabrics\\nForeign objects (e.g. coins, paper clips, \\nneedles, nails) may damage the laundry or components in the appliance. \\nTherefore, note the following tips when \\npreparing your laundry:\\n■Empty any pockets.\\n■Check for metal items (paper clips, etc.) and remove them.\\n■Wash delicates in a laundry bag (tights, underwired bras, etc.).\\n■Remove curtain fittings or place curtains in a laundry bag.\\n■Close any zips, button up any cover buttons.\\n■Brush sand out of pockets and collars.Sorting laundry\\nSort your laundry according to the care instructions and manufacturer's information on the care labels:\\n■Type of fabric/fibre\\n■Colour\\nNote:  Laundry can discolour or not \\nbe cleaned correctly. Wash white and coloured laundry separately.Wash new bright fabrics separately the first time you wash them.\\n■SoilingWash laundry with the same level of soiling together.You can find examples of levels of soiling on –Light: Do not pretreat, select the \\nG SpeedPerfect  setting if \\nnecessary\\n–Normal\\n–Heavy : Load less laundry, \\npretreat the laundry if necessary.\\n–Stains : Remove/pretreat stains \\nwhile they are fresh. First, dab the stain with soapy water/do not rub. Then wash the items of laundry with the appropriate programme. Stubborn/dried-in stains can sometimes only be removed with several washes.\\n■Symbols on the care labelsThe numbers in the symbols indicate the maximum washing temperature which may be used.–M: Suitable for a normal \\nwashing process; e.g. i Cottons  programme.\\n–U: A gentle washing process is \\nrequired; \\ne.g. = Easy Care//Synthetics  \\nprogramme.\\n–V: An especially gentle washing \\nprocess is required; \\ne.g. “ Delicates/Silk//Delicate/\\nSilk programme.\\n–W: Suitable for washing by hand; \\ne.g. WãWool programme.\\n–Ž: Do not wash the laundry in \\nthe machine.\", metadata={'source': './data/9001408066_B.pdf', 'page': 10})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we check one of the documents we can see the page_content and metadata, in this case the document name and page number but we could define more\n",
    "data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 400\n",
    "chunk_overlap = 30\n",
    "# We use the RecursiveCharacterTextSplitter but there are others as well \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents after split: 184\n"
     ]
    }
   ],
   "source": [
    "print(f'Total documents after split: {len(docs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Starting the programme  . . . . . . . . . .18Childproof lock . . . . . . . . . . . . . . . . .19Adding/removing laundry . . . . . . . . .19Changing the programme . . . . . . . . .19Cancel the programme . . . . . . . . . . .20Programme end during rinse hold. . .20Programme end  . . . . . . . . . . . . . . . .20Removing laundry/switching off the', metadata={'source': './data/9001408066_B.pdf', 'page': 2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not much has changed to the format except that the page content is now smaller due to the splitting\n",
    "docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading embeddings\n",
    "\n",
    "We use the HuggingFace Instructor Embeddings but we could also use other embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ellenhoeven/Repositories/gen-ai/side-projects/genai-starterkit/.venv/lib/python3.11/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"hkunlp/instructor-large\",\n",
    "            model_kwargs={\"device\": \"cpu\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceInstructEmbeddings(client=INSTRUCTOR(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: T5EncoderModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False})\n",
       "  (2): Dense({'in_features': 1024, 'out_features': 768, 'bias': False, 'activation_function': 'torch.nn.modules.linear.Identity'})\n",
       "  (3): Normalize()\n",
       "), model_name='hkunlp/instructor-large', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={}, embed_instruction='Represent the document for retrieval: ', query_instruction='Represent the question for retrieving supporting documents: ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking what is in the embeddings\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the embeddings into a vector store\n",
    "\n",
    "For now we use FAISS in-memory as vector index but this may be changed out in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x2a0407e90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the db\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_FAISS__add',\n",
       " '_FAISS__from',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_aembed_documents',\n",
       " '_aembed_query',\n",
       " '_asimilarity_search_with_relevance_scores',\n",
       " '_cosine_relevance_score_fn',\n",
       " '_embed_documents',\n",
       " '_embed_query',\n",
       " '_euclidean_relevance_score_fn',\n",
       " '_get_retriever_tags',\n",
       " '_max_inner_product_relevance_score_fn',\n",
       " '_normalize_L2',\n",
       " '_select_relevance_score_fn',\n",
       " '_similarity_search_with_relevance_scores',\n",
       " 'aadd_documents',\n",
       " 'aadd_texts',\n",
       " 'add_documents',\n",
       " 'add_embeddings',\n",
       " 'add_texts',\n",
       " 'adelete',\n",
       " 'afrom_documents',\n",
       " 'afrom_embeddings',\n",
       " 'afrom_texts',\n",
       " 'amax_marginal_relevance_search',\n",
       " 'amax_marginal_relevance_search_by_vector',\n",
       " 'amax_marginal_relevance_search_with_score_by_vector',\n",
       " 'as_retriever',\n",
       " 'asearch',\n",
       " 'asimilarity_search',\n",
       " 'asimilarity_search_by_vector',\n",
       " 'asimilarity_search_with_relevance_scores',\n",
       " 'asimilarity_search_with_score',\n",
       " 'asimilarity_search_with_score_by_vector',\n",
       " 'delete',\n",
       " 'deserialize_from_bytes',\n",
       " 'distance_strategy',\n",
       " 'docstore',\n",
       " 'embedding_function',\n",
       " 'embeddings',\n",
       " 'from_documents',\n",
       " 'from_embeddings',\n",
       " 'from_texts',\n",
       " 'index',\n",
       " 'index_to_docstore_id',\n",
       " 'load_local',\n",
       " 'max_marginal_relevance_search',\n",
       " 'max_marginal_relevance_search_by_vector',\n",
       " 'max_marginal_relevance_search_with_score_by_vector',\n",
       " 'merge_from',\n",
       " 'override_relevance_score_fn',\n",
       " 'save_local',\n",
       " 'search',\n",
       " 'serialize_to_bytes',\n",
       " 'similarity_search',\n",
       " 'similarity_search_by_vector',\n",
       " 'similarity_search_with_relevance_scores',\n",
       " 'similarity_search_with_score',\n",
       " 'similarity_search_with_score_by_vector']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see, there are multiple built in functions for the similarity search\n",
    "# But also for adding documents, and we can also save the db locally\n",
    "dir(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see what happens\n",
    "# db.save_local('output/vectordb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Faults and what to do about them    en\n",
      "27Faults and what to do about them\n",
      "Faults Cause/Remedy\n",
      "Water is leaking from the \n",
      "machine.■Attach the drain hose correctly or replace it.\n",
      "■Tighten the screw connection of the drain hose.\n",
      "The machine is not filling with water.\n",
      "Detergent is not being dis-\n",
      "pensed.■Have you pressed the A button?\n",
      "■Is the tap turned on?\n",
      "----\n",
      "immersed in the drained water, water may be sucked back into the appliance and may damage the appliance/fabrics.Make sure that: – The plug does not block drainage \n",
      "from the sink.\n",
      "– The end of the drain hose is not \n",
      "immersed in the drained water.\n",
      "– The water drains away quickly \n",
      "enough.Secure the outlet hose so that it\n",
      "----\n",
      "damage. Do not open the washing machine door if water can be seen through the glass.If the laundry has to be removed, the \n",
      "washing machine door can be opened as follows:1.Switch off the appliance.\n",
      "2.Disconnect the mains plug.\n",
      "3.Drain the water. ~ Page 23\n",
      "4.Pull the emergency release \n",
      "downwards with a tool and release.The washing machine door can now be opened.\n"
     ]
    }
   ],
   "source": [
    "# The FAISS db has a built-in similarity search function that we can use\n",
    "query = \"What do I do if water is leaking?\"\n",
    "docs = db.similarity_search(query, k=3)\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n",
    "print(\"----\")\n",
    "print(docs[1].page_content)\n",
    "print(\"----\")\n",
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the LLM Interface\n",
    "\n",
    "Now that we have the results from our similarity search, we use these and add them to our prompt to send to the LLM\n",
    "\n",
    "For now we will use the OpenAI API to get started quickly but this will definitely be a different LLM (API or local) in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we use mainly the default settings\n",
    "# TODO add env file for OpenAI Key\n",
    "# For now you need to do export OPENAI_API_KEY=''\n",
    "model_llm = OpenAI(api_key=\"\", model=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our chain\n",
    "\n",
    "A Chain in LangChain is something that glues together operations.\n",
    "\n",
    "There is a simple sequential chain, but there are also targeted chains, for instance for QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "# Note that there are multiple chain types in langchain.\n",
    "# retrieval qa and load qa are quite similar\n",
    "\n",
    "\n",
    "qa = load_qa_chain(llm=model_llm, chain_type=\"stuff\")\n",
    "\n",
    "# This takes some default settings for the retriever\n",
    "# qa = RetrievalQA.from_chain_type(llm=model_llm, \n",
    "#                                  chain_type=\"stuff\", \n",
    "#                                  retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "query = \"What do I do if water is leaking?\"\n",
    "ss_docs = db.similarity_search(query, k=3)\n",
    "print(len(ss_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Attach the drain hose correctly or replace it, and tighten the screw connection of the drain hose. Make sure that the plug does not block drainage from the sink, and that the end of the drain hose is not immersed in the drained water.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(input_documents=ss_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
